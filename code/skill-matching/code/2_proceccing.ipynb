{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os,sys,re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm.autonotebook import tqdm\n",
    "from time import sleep\n",
    "\n",
    "# import openpyxl\n",
    "\n",
    "import json\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from janome.tokenizer import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# 形態要素解析とストップワードの削除\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#base-path\n",
    "path = \"/workspace\"\n",
    "data_path = os.path.join(path,\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "cv_df = pd.read_csv(os.path.join(data_path,\"input/resume_corpus.csv\"))\n",
    "JDs_df = pd.read_csv(os.path.join(data_path,\"input/Train_rev1.csv\"))\n",
    "JDs_df = JDs_df.query(\"Category == 'Engineering Jobs'\")\n",
    "JDs_df = JDs_df.reset_index(drop=True)\n",
    "DE_JDs_df = pd.read_csv(os.path.join(data_path,'input/DataEngineer.csv'))\n",
    "cv_df = cv_df.reset_index(drop=True)\n",
    "cv_df = cv_df.loc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストの処理からスキル抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#テキストの前処理の関数\n",
    "def re_clean(_df):\n",
    "    \n",
    "    _dict = _df.to_dict()\n",
    "    for key in _dict.keys():\n",
    "        #小文字化\n",
    "        _txt = str(_dict[key])\n",
    "        _txt = _txt.lower()\n",
    "        #cvのhtmlタグの除去\n",
    "        _txt = re.sub(re.compile('<.*?>'), '', _txt)\n",
    "\n",
    "        #urlの削除\n",
    "        _txt = re.sub(\"https?://[\\w!\\?/\\+\\-_~=;\\.,\\*&@#\\$%\\(\\)'\\[\\]]+\", \"\", _txt)\n",
    "\n",
    "        #特殊文字の削除\n",
    "        _txt = re.sub(r\"[^a-zA-Z0-9]\",\" \",_txt)\n",
    "        _txt = re.sub(r\"\\s+\", ' ', _txt)\n",
    "\n",
    "        #数字を全て0に\n",
    "        _dict[key] = re.sub(r\"\\d+\",\" \",_txt)\n",
    "    return _dict \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# load default skills data base\n",
    "from skillNer.general_params import SKILL_DB\n",
    "# import skill extractor\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_skiller(_dict):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    error_count = []\n",
    "    count = 0\n",
    "# init skill extractor\n",
    "    skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)\n",
    "    skill_dict = {}\n",
    "    error_dict = {}\n",
    "    for key in tqdm(list(_dict.keys())):\n",
    "        try:\n",
    "            annotations = skill_extractor.annotate(_dict[key])\n",
    "            skill_list = []\n",
    "           \n",
    "            dict_list = {}\n",
    "            for i in range(len(annotations[\"results\"]['full_matches'])):\n",
    "                skill_list.append( annotations[\"results\"][\"full_matches\"][i][\"doc_node_value\"])\n",
    "\n",
    "            for j in range(len(annotations[\"results\"]['ngram_scored'])):\n",
    "                skill_list.append(annotations[\"results\"][\"ngram_scored\"][j][\"doc_node_value\"])\n",
    "                dict_list[\"skill\"] = skill_list\n",
    "            _dict[key] = dict_list\n",
    "        except:\n",
    "            error_count.append(key)\n",
    "            _dict.pop(key)\n",
    "            error_dict[key] = 1 \n",
    "            count = count+1\n",
    "            continue\n",
    "    print(count)\n",
    "    return _dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #スキルの抽出\n",
    "# #全部終えるのに15時間以上かかります。\n",
    "cv_skills =  get_skiller(re_clean(cv_df.cv))\n",
    "JDs_skills = get_skiller(re_clean(JDs_df.FullDescription))\n",
    "DE_skills = get_skiller(re_clean(DE_JDs_df[\"Job Description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#格納用のディレクトリの作成\n",
    "!mkdir -p /workspace/data/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path,'output/skills_cv_100.json'),\"w\") as f:\n",
    "    json.dump(cv_skills,f)\n",
    "\n",
    "with open(os.path.join(data_path,'output/skills_JDs_1000.json'),\"w\") as f:\n",
    "    json.dump(JDs_skills,f)\n",
    "\n",
    "with open(os.path.join(data_path,r'output/skills_DataEngineer_JDs_2528.json'),\"w\") as f:\n",
    "    json.dump(DE_skills,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態要素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#抽出したskillのロード\n",
    "with open(os.path.join(data_path,r'output/skills_cv_100.json'),\"r\") as f:\n",
    "    skill_cv = json.load(f)\n",
    "\n",
    "with open(os.path.join(data_path,r'output/skills_JDs_1000.json'),\"r\") as f:\n",
    "    skill_JDs = json.load(f)\n",
    "\n",
    "with open(os.path.join(data_path,r'output/skills_DataEngineer_JDs_2528.json'),\"r\") as f:\n",
    "    skill_DE = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#skillを全て小文字に\n",
    "def lower(x):\n",
    "    return x.lower()\n",
    "for key,val in skill_DE.items():\n",
    "    try:\n",
    "        val[\"skill\"] = list(map(lower,val[\"skill\"]))\n",
    "    except:\n",
    "        continue\n",
    "for key,val in skill_JDs.items():\n",
    "    try:\n",
    "        val[\"skill\"] = list(map(lower,val[\"skill\"]))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for key,val in skill_cv.items():\n",
    "    try:\n",
    "        val[\"skill\"] = list(map(lower,val[\"skill\"]))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ストップワードのロード\n",
    "from nltk.util import ngrams\n",
    "from janome.tokenizer import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#テキスト処理\n",
    "def re_clean_text(_text):\n",
    "\n",
    "    #小文字化\n",
    "    _txt = _text.lower()\n",
    "    #cvのhtmlタグの除去\n",
    "    _txt = re.sub(re.compile('<.*?>'), '', _txt)\n",
    "\n",
    "    #urlの削除\n",
    "    _txt = re.sub(\"https?://[\\w!\\?/\\+\\-_~=;\\.,\\*&@#\\$%\\(\\)'\\[\\]]+\", \"\", _txt)\n",
    "\n",
    "    #特殊文字の削除\n",
    "    _txt = re.sub(r\"[^a-zA-Z0-9]\",\" \",_txt)\n",
    "    _txt = re.sub(r\"\\s+\", ' ', _txt)\n",
    "\n",
    "    #数字を全て0に\n",
    "    _txt = re.sub(r\"\\d+\",\" \",_txt)\n",
    "    return _txt\n",
    "\n",
    "#トークン化とストップワードの除去\n",
    "def Token(_text):\n",
    "    cp_text = str(_text)\n",
    "    t = Tokenizer(wakati=True)\n",
    "    token_lists = []\n",
    "    for token in t.tokenize(cp_text):\n",
    "        if ((token.strip() != '')):\n",
    "            token_lists.append(token)\n",
    "    # ストップワードの除去\n",
    "    filtered_sentence = []\n",
    "    for w in token_lists:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    cp_text = filtered_sentence\n",
    "\n",
    "    return cp_text\n",
    "\n",
    "#2-gramとして処理し、\"deep learning\"のような抽出されたスキル単語を1つのトークンとして扱うように処理する\n",
    "\n",
    "def re_token(_token, skill_list):\n",
    "\n",
    "    each_bigrams_list = [(_token[i], _token[i+1]) for i in range(len(_token)-1)]  # bigramsの作成\n",
    "    prcc_list = (len(_token)-1)*[0]\n",
    "    # 結合した文字がもしskillにあるならばskillに置き換えて1単語として扱う\n",
    "    for i in range(len(each_bigrams_list)):\n",
    "        join_word = \" \".join(each_bigrams_list[i])  # bigramごとに一度文字をjoin\n",
    "        if join_word in skill_list:\n",
    "            in_list = [*each_bigrams_list[i]].copy()\n",
    "            in_list.append(join_word)\n",
    "            del in_list[:-1]\n",
    "            [*each_bigrams_list[i]] = in_list\n",
    "            prcc_list[i] = [*each_bigrams_list[i]][0]\n",
    "        prcc_list[i] = [*each_bigrams_list[i]][-1]\n",
    "    return prcc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def marge_json(skill_dict, id_and_full_text_df):\n",
    "    # keyをidに\n",
    "    id_skill_cv = {}\n",
    "    _dict = skill_dict.copy\n",
    "    for key_index in list(skill_dict.keys()):\n",
    "        id = id_and_full_text_df.iloc[int(key_index)-1,0]\n",
    "        try:\n",
    "            value_dict = {}\n",
    "            value_dict[\"text\"] = re_clean_text(id_and_full_text_df.iloc[int(key_index),1])  # textを追加\n",
    "            value_dict[\"skill\"] = skill_dict[key_index][\"skill\"]  # keyをindexからidに変更\n",
    "            value_dict[\"unique_skill\"] = sorted(set(\n",
    "                value_dict['skill']), key=value_dict['skill'].index)  # skillをuniqueに\n",
    "            value_dict[\"token\"] = Token(value_dict[\"text\"])  # tokenを追加\n",
    "            value_dict[\"re_token\"] = re_token(\n",
    "                value_dict[\"token\"], value_dict[\"skill\"])  # skillを考慮したtoken\n",
    "            id_skill_cv[str(id)] = value_dict\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return id_skill_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#格納用のディレクトリの作成\n",
    "!mkdir -p /workspace/data/output/marge_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dt_now = datetime.datetime.now()\n",
    "#dump\n",
    "with open(os.path.join(data_path,\"output/marge_json/marge_cv.json\"),\"w\") as f:\n",
    "    json.dump(marge_json(skill_cv,cv_df.iloc[:,[0,11]]),f)\n",
    "\n",
    "with open(os.path.join(data_path,\"output/marge_json/marge_JDs.json\"),\"w\") as f:\n",
    "    json.dump(marge_json(skill_JDs,JDs_df.iloc[:,[0,2]]),f)\n",
    "\n",
    "with open(os.path.join(data_path,\"output/marge_json/marge_DataEngineer.json\"),\"w\") as f:\n",
    "    json.dump(marge_json(skill_DE,DE_JDs_df.iloc[:,[0,3]]),f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "08bab3d115f66d421a5714c32a33264a27cd931d9c4fac1dbdcf754479af7214"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
